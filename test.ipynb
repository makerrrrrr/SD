{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Diffusion(\n",
    "  (time_embedding): TimeEmbedding(\n",
    "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
    "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "  )\n",
    "  (unet): UNet(\n",
    "    (encoders): ModuleList(\n",
    "      (0): SwitchSequential(\n",
    "        (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      )\n",
    "      (1-2): 2 x SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=320, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Identity()\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=320, out_features=960, bias=False)\n",
    "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=320, out_features=320, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=320, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=320, bias=False)\n",
    "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=320, out_features=2560, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=1280, out_features=320, bias=True)\n",
    "          (conv_output): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (3): SwitchSequential(\n",
    "        (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      )\n",
    "      (4): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
    "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (5): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Identity()\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
    "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (6): SwitchSequential(\n",
    "        (0): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      )\n",
    "      (7): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
    "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
    "          (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (8): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Identity()\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
    "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
    "          (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (9): SwitchSequential(\n",
    "        (0): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "      )\n",
    "      (10-11): 2 x SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Identity()\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (bottleneck): SwitchSequential(\n",
    "      (0): ResidualBlock(\n",
    "        (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "        (conv_feature): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "        (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "        (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (residual_layer): Identity()\n",
    "      )\n",
    "      (1): AttentionBlock(\n",
    "        (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
    "        (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "        (attention_1): SelfAttention(\n",
    "          (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
    "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "        )\n",
    "        (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "        (attention_2): CrossAttention(\n",
    "          (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
    "          (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "          (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "          (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "        )\n",
    "        (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "        (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
    "        (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
    "        (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "      )\n",
    "      (2): ResidualBlock(\n",
    "        (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "        (conv_feature): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "        (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "        (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        (residual_layer): Identity()\n",
    "      )\n",
    "    )\n",
    "    (decoders): ModuleList(\n",
    "      (0-1): 2 x SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (2): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): Upsample(\n",
    "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (3-4): 2 x SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
    "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
    "          (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (5): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=1280, out_features=3840, bias=False)\n",
    "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=1280, bias=False)\n",
    "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=1280, out_features=10240, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
    "          (conv_output): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (2): Upsample(\n",
    "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (6): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
    "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (7): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
    "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (8): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=640, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=640, out_features=1920, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=640, out_features=640, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=640, bias=False)\n",
    "            (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=640, out_features=5120, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=2560, out_features=640, bias=True)\n",
    "          (conv_output): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (2): Upsample(\n",
    "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (9): SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=320, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=320, out_features=960, bias=False)\n",
    "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=320, out_features=320, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=320, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=320, bias=False)\n",
    "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=320, out_features=2560, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=1280, out_features=320, bias=True)\n",
    "          (conv_output): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "      (10-11): 2 x SwitchSequential(\n",
    "        (0): ResidualBlock(\n",
    "          (groupnorm_feature): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
    "          (conv_feature): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (linear_time): Linear(in_features=1280, out_features=320, bias=True)\n",
    "          (groupnorm_merged): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
    "          (conv_merged): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "          (residual_layer): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "        (1): AttentionBlock(\n",
    "          (groupnorm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
    "          (conv_input): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
    "          (layernorm_1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_1): SelfAttention(\n",
    "            (in_proj): Linear(in_features=320, out_features=960, bias=False)\n",
    "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
    "          )\n",
    "          (layernorm_2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (attention_2): CrossAttention(\n",
    "            (q_proj): Linear(in_features=320, out_features=320, bias=False)\n",
    "            (k_proj): Linear(in_features=768, out_features=320, bias=False)\n",
    "            (v_proj): Linear(in_features=768, out_features=320, bias=False)\n",
    "            (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
    "          )\n",
    "          (layernorm_3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
    "          (linear_geglu_1): Linear(in_features=320, out_features=2560, bias=True)\n",
    "          (linear_geglu_2): Linear(in_features=1280, out_features=320, bias=True)\n",
    "          (conv_output): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (final): FinalLayer(\n",
    "    (groupnorm): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
    "    (conv): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([999.0000, 978.6122, 958.2245, 937.8367, 917.4490, 897.0612, 876.6735,\n",
       "        856.2857, 835.8979, 815.5102, 795.1224, 774.7347, 754.3469, 733.9592,\n",
       "        713.5714, 693.1837, 672.7959, 652.4082, 632.0204, 611.6327, 591.2449,\n",
       "        570.8572, 550.4694, 530.0817, 509.6939, 489.3061, 468.9184, 448.5306,\n",
       "        428.1429, 407.7551, 387.3673, 366.9796, 346.5918, 326.2041, 305.8163,\n",
       "        285.4286, 265.0408, 244.6530, 224.2653, 203.8775, 183.4898, 163.1020,\n",
       "        142.7143, 122.3265, 101.9388,  81.5510,  61.1633,  40.7755,  20.3878,\n",
       "          0.0000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "timesteps=torch.linspace(999,0,50)\n",
    "timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.38779999999997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "999.0000-978.6122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.387699999999995"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "978.6122-958.2245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.387800000000084"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "958.2245-937.8367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.387700000000002"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "81.5510-61.1633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.arange(start=0, end=5, dtype=dtype) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2000, 0.4000, 0.6000, 0.8000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs=torch.pow(10000,-torch.arange(start=0, end=5, dtype=dtype) / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.5849e-01, 2.5119e-02, 3.9811e-03, 6.3096e-04])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = np.float64(999.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6310, 0.3981, 0.2512, 0.1585])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(10,-torch.arange(0,5)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6310, 0.3981, 0.2512, 0.1585])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype=torch.float32\n",
    "# torch.pow(10,-torch.arange(0,5,dtype=dtype)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=-torch.arange(0,5,dtype=dtype)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -0.2000, -0.4000, -0.6000, -0.8000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs=torch.pow(10,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6310, 0.3981, 0.2512, 0.1585])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([2])\n",
    "x.shape\n",
    "x1=x[:,None]\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个 3D 张量\n",
    "tensor = torch.arange(27).reshape(3, 3, 3)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 10, 11],\n",
       "        [12, 13, 14],\n",
       "        [15, 16, 17]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#单维度切片\n",
    "#选择第0维，索引为1\n",
    "x=tensor[1]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1],\n",
      "         [ 3,  4],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [12, 13],\n",
      "         [15, 16]]])\n"
     ]
    }
   ],
   "source": [
    "#多维度切片\n",
    "# 选择前两层（0 和 1），所有行，列取 0 和 1\n",
    "result = tensor[:2, :, :2]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原张量 x： tensor([1, 2, 3])\n",
      "形状： torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])  # 一维张量\n",
    "print(\"原张量 x：\", x)\n",
    "print(\"形状：\", x.shape)  # torch.Size([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扩展后的张量 x_expanded：\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "形状： torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "x_expanded = x[:, None]\n",
    "print(\"扩展后的张量 x_expanded：\\n\", x_expanded)\n",
    "print(\"形状：\", x_expanded.shape)  # torch.Size([3, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "b = torch.tensor([[7, 8, 9],\n",
    "                  [10, 11, 12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "result_dim0 = torch.cat([a, b], dim=0)\n",
    "print(result_dim0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  7,  8,  9],\n",
      "        [ 4,  5,  6, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "result_dim1 = torch.cat([a, b], dim=-1)\n",
    "print(result_dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[ 1,  2,  3,  7,  8,  9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls+=[10,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls+=[0,-1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 7, 8, 9, 10, 11, 0, -1, 0, -1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.38775510000005"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "999-978.6122449 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.387755099999936"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "978.6122449-958.2244898"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
